{"ast":null,"code":"// +----------------------------------------------------------------------+\n// | murmurHash3js.js v3.0.1 // https://github.com/pid/murmurHash3js\n// | A javascript implementation of MurmurHash3's x86 hashing algorithms. |\n// |----------------------------------------------------------------------|\n// | Copyright (c) 2012-2015 Karan Lyons                                       |\n// | https://github.com/karanlyons/murmurHash3.js/blob/c1778f75792abef7bdd74bc85d2d4e1a3d25cfe9/murmurHash3.js |\n// | Freely distributable under the MIT license.                          |\n// +----------------------------------------------------------------------+\n// PRIVATE FUNCTIONS\n// -----------------\nfunction _x86Multiply(m, n) {\n  //\n  // Given two 32bit ints, returns the two multiplied together as a\n  // 32bit int.\n  //\n  return (m & 0xffff) * n + (((m >>> 16) * n & 0xffff) << 16);\n}\nfunction _x86Rotl(m, n) {\n  //\n  // Given a 32bit int and an int representing a number of bit positions,\n  // returns the 32bit int rotated left by that number of positions.\n  //\n  return m << n | m >>> 32 - n;\n}\nfunction _x86Fmix(h) {\n  //\n  // Given a block, returns murmurHash3's final x86 mix of that block.\n  //\n  h ^= h >>> 16;\n  h = _x86Multiply(h, 0x85ebca6b);\n  h ^= h >>> 13;\n  h = _x86Multiply(h, 0xc2b2ae35);\n  h ^= h >>> 16;\n  return h;\n}\nfunction _x64Add(m, n) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // added together as a 64bit int (as an array of two 32bit ints).\n  //\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n  o[3] += m[3] + n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n  o[2] += m[2] + n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n  o[1] += m[1] + n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[0] += m[0] + n[0];\n  o[0] &= 0xffff;\n  return [o[0] << 16 | o[1], o[2] << 16 | o[3]];\n}\nfunction _x64Multiply(m, n) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // multiplied together as a 64bit int (as an array of two 32bit ints).\n  //\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n  o[3] += m[3] * n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n  o[2] += m[2] * n[3];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n  o[2] += m[3] * n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n  o[1] += m[1] * n[3];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[1] += m[2] * n[2];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[1] += m[3] * n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0];\n  o[0] &= 0xffff;\n  return [o[0] << 16 | o[1], o[2] << 16 | o[3]];\n}\nfunction _x64Rotl(m, n) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) rotated left by that number of positions.\n  //\n  n %= 64;\n  if (n === 32) {\n    return [m[1], m[0]];\n  } else if (n < 32) {\n    return [m[0] << n | m[1] >>> 32 - n, m[1] << n | m[0] >>> 32 - n];\n  } else {\n    n -= 32;\n    return [m[1] << n | m[0] >>> 32 - n, m[0] << n | m[1] >>> 32 - n];\n  }\n}\nfunction _x64LeftShift(m, n) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) shifted left by that number of positions.\n  //\n  n %= 64;\n  if (n === 0) {\n    return m;\n  } else if (n < 32) {\n    return [m[0] << n | m[1] >>> 32 - n, m[1] << n];\n  } else {\n    return [m[1] << n - 32, 0];\n  }\n}\nfunction _x64Xor(m, n) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // xored together as a 64bit int (as an array of two 32bit ints).\n  //\n  return [m[0] ^ n[0], m[1] ^ n[1]];\n}\nfunction _x64Fmix(h) {\n  //\n  // Given a block, returns murmurHash3's final x64 mix of that block.\n  // (`[0, h[0] >>> 1]` is a 33 bit unsigned right shift. This is the\n  // only place where we need to right shift 64bit ints.)\n  //\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xff51afd7, 0xed558ccd]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xc4ceb9fe, 0x1a85ec53]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  return h;\n}\n// PUBLIC FUNCTIONS\n// ----------------\nfunction x86Hash32(bytes, seed) {\n  //\n  // Given a string and an optional seed as an int, returns a 32 bit hash\n  // using the x86 flavor of MurmurHash3, as an unsigned int.\n  //\n  seed = seed || 0;\n  const remainder = bytes.length % 4;\n  const blocks = bytes.length - remainder;\n  let h1 = seed;\n  let k1 = 0;\n  const c1 = 0xcc9e2d51;\n  const c2 = 0x1b873593;\n  let j = 0;\n  for (let i = 0; i < blocks; i = i + 4) {\n    k1 = bytes[i] | bytes[i + 1] << 8 | bytes[i + 2] << 16 | bytes[i + 3] << 24;\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n    h1 ^= k1;\n    h1 = _x86Rotl(h1, 13);\n    h1 = _x86Multiply(h1, 5) + 0xe6546b64;\n    j = i + 4;\n  }\n  k1 = 0;\n  switch (remainder) {\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n  h1 ^= bytes.length;\n  h1 = _x86Fmix(h1);\n  return h1 >>> 0;\n}\nfunction x86Hash128(bytes, seed) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x86 flavor of MurmurHash3, as an unsigned hex.\n  //\n  seed = seed || 0;\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n  let h1 = seed;\n  let h2 = seed;\n  let h3 = seed;\n  let h4 = seed;\n  let k1 = 0;\n  let k2 = 0;\n  let k3 = 0;\n  let k4 = 0;\n  const c1 = 0x239b961b;\n  const c2 = 0xab0e9789;\n  const c3 = 0x38b34ae5;\n  const c4 = 0xa1e38b93;\n  let j = 0;\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = bytes[i] | bytes[i + 1] << 8 | bytes[i + 2] << 16 | bytes[i + 3] << 24;\n    k2 = bytes[i + 4] | bytes[i + 5] << 8 | bytes[i + 6] << 16 | bytes[i + 7] << 24;\n    k3 = bytes[i + 8] | bytes[i + 9] << 8 | bytes[i + 10] << 16 | bytes[i + 11] << 24;\n    k4 = bytes[i + 12] | bytes[i + 13] << 8 | bytes[i + 14] << 16 | bytes[i + 15] << 24;\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n    h1 ^= k1;\n    h1 = _x86Rotl(h1, 19);\n    h1 += h2;\n    h1 = _x86Multiply(h1, 5) + 0x561ccd1b;\n    k2 = _x86Multiply(k2, c2);\n    k2 = _x86Rotl(k2, 16);\n    k2 = _x86Multiply(k2, c3);\n    h2 ^= k2;\n    h2 = _x86Rotl(h2, 17);\n    h2 += h3;\n    h2 = _x86Multiply(h2, 5) + 0x0bcaa747;\n    k3 = _x86Multiply(k3, c3);\n    k3 = _x86Rotl(k3, 17);\n    k3 = _x86Multiply(k3, c4);\n    h3 ^= k3;\n    h3 = _x86Rotl(h3, 15);\n    h3 += h4;\n    h3 = _x86Multiply(h3, 5) + 0x96cd1c35;\n    k4 = _x86Multiply(k4, c4);\n    k4 = _x86Rotl(k4, 18);\n    k4 = _x86Multiply(k4, c1);\n    h4 ^= k4;\n    h4 = _x86Rotl(h4, 13);\n    h4 += h1;\n    h4 = _x86Multiply(h4, 5) + 0x32ac3b17;\n    j = i + 16;\n  }\n  k1 = 0;\n  k2 = 0;\n  k3 = 0;\n  k4 = 0;\n  switch (remainder) {\n    case 15:\n      k4 ^= bytes[j + 14] << 16;\n    case 14:\n      k4 ^= bytes[j + 13] << 8;\n    case 13:\n      k4 ^= bytes[j + 12];\n      k4 = _x86Multiply(k4, c4);\n      k4 = _x86Rotl(k4, 18);\n      k4 = _x86Multiply(k4, c1);\n      h4 ^= k4;\n    case 12:\n      k3 ^= bytes[j + 11] << 24;\n    case 11:\n      k3 ^= bytes[j + 10] << 16;\n    case 10:\n      k3 ^= bytes[j + 9] << 8;\n    case 9:\n      k3 ^= bytes[j + 8];\n      k3 = _x86Multiply(k3, c3);\n      k3 = _x86Rotl(k3, 17);\n      k3 = _x86Multiply(k3, c4);\n      h3 ^= k3;\n    case 8:\n      k2 ^= bytes[j + 7] << 24;\n    case 7:\n      k2 ^= bytes[j + 6] << 16;\n    case 6:\n      k2 ^= bytes[j + 5] << 8;\n    case 5:\n      k2 ^= bytes[j + 4];\n      k2 = _x86Multiply(k2, c2);\n      k2 = _x86Rotl(k2, 16);\n      k2 = _x86Multiply(k2, c3);\n      h2 ^= k2;\n    case 4:\n      k1 ^= bytes[j + 3] << 24;\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n  h1 ^= bytes.length;\n  h2 ^= bytes.length;\n  h3 ^= bytes.length;\n  h4 ^= bytes.length;\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n  h1 = _x86Fmix(h1);\n  h2 = _x86Fmix(h2);\n  h3 = _x86Fmix(h3);\n  h4 = _x86Fmix(h4);\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n  return (\"00000000\" + (h1 >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h2 >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h3 >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h4 >>> 0).toString(16)).slice(-8);\n}\nfunction x64Hash128(bytes, seed) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x64 flavor of MurmurHash3, as an unsigned hex.\n  //\n  seed = seed || 0;\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n  let h1 = [0, seed];\n  let h2 = [0, seed];\n  let k1 = [0, 0];\n  let k2 = [0, 0];\n  const c1 = [0x87c37b91, 0x114253d5];\n  const c2 = [0x4cf5ad43, 0x2745937f];\n  let j = 0;\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = [bytes[i + 4] | bytes[i + 5] << 8 | bytes[i + 6] << 16 | bytes[i + 7] << 24, bytes[i] | bytes[i + 1] << 8 | bytes[i + 2] << 16 | bytes[i + 3] << 24];\n    k2 = [bytes[i + 12] | bytes[i + 13] << 8 | bytes[i + 14] << 16 | bytes[i + 15] << 24, bytes[i + 8] | bytes[i + 9] << 8 | bytes[i + 10] << 16 | bytes[i + 11] << 24];\n    k1 = _x64Multiply(k1, c1);\n    k1 = _x64Rotl(k1, 31);\n    k1 = _x64Multiply(k1, c2);\n    h1 = _x64Xor(h1, k1);\n    h1 = _x64Rotl(h1, 27);\n    h1 = _x64Add(h1, h2);\n    h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 0x52dce729]);\n    k2 = _x64Multiply(k2, c2);\n    k2 = _x64Rotl(k2, 33);\n    k2 = _x64Multiply(k2, c1);\n    h2 = _x64Xor(h2, k2);\n    h2 = _x64Rotl(h2, 31);\n    h2 = _x64Add(h2, h1);\n    h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 0x38495ab5]);\n    j = i + 16;\n  }\n  k1 = [0, 0];\n  k2 = [0, 0];\n  switch (remainder) {\n    case 15:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 14]], 48));\n    case 14:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 13]], 40));\n    case 13:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 12]], 32));\n    case 12:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 11]], 24));\n    case 11:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 10]], 16));\n    case 10:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 9]], 8));\n    case 9:\n      k2 = _x64Xor(k2, [0, bytes[j + 8]]);\n      k2 = _x64Multiply(k2, c2);\n      k2 = _x64Rotl(k2, 33);\n      k2 = _x64Multiply(k2, c1);\n      h2 = _x64Xor(h2, k2);\n    case 8:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 7]], 56));\n    case 7:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 6]], 48));\n    case 6:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 5]], 40));\n    case 5:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 4]], 32));\n    case 4:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 3]], 24));\n    case 3:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 2]], 16));\n    case 2:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 1]], 8));\n    case 1:\n      k1 = _x64Xor(k1, [0, bytes[j]]);\n      k1 = _x64Multiply(k1, c1);\n      k1 = _x64Rotl(k1, 31);\n      k1 = _x64Multiply(k1, c2);\n      h1 = _x64Xor(h1, k1);\n  }\n  h1 = _x64Xor(h1, [0, bytes.length]);\n  h2 = _x64Xor(h2, [0, bytes.length]);\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n  h1 = _x64Fmix(h1);\n  h2 = _x64Fmix(h2);\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n  // Here we reverse h1 and h2 in Cosmos\n  // This is an implementation detail and not part of the public spec\n  const h1Buff = Buffer.from((\"00000000\" + (h1[0] >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h1[1] >>> 0).toString(16)).slice(-8), \"hex\");\n  const h1Reversed = reverse(h1Buff).toString(\"hex\");\n  const h2Buff = Buffer.from((\"00000000\" + (h2[0] >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h2[1] >>> 0).toString(16)).slice(-8), \"hex\");\n  const h2Reversed = reverse(h2Buff).toString(\"hex\");\n  return h1Reversed + h2Reversed;\n}\nexport function reverse(buff) {\n  const buffer = Buffer.allocUnsafe(buff.length);\n  for (let i = 0, j = buff.length - 1; i <= j; ++i, --j) {\n    buffer[i] = buff[j];\n    buffer[j] = buff[i];\n  }\n  return buffer;\n}\nexport default {\n  version: \"3.0.0\",\n  x86: {\n    hash32: x86Hash32,\n    hash128: x86Hash128\n  },\n  x64: {\n    hash128: x64Hash128\n  },\n  inputValidation: true\n};","map":{"version":3,"names":["_x86Multiply","m","n","_x86Rotl","_x86Fmix","h","_x64Add","o","_x64Multiply","_x64Rotl","_x64LeftShift","_x64Xor","_x64Fmix","x86Hash32","bytes","seed","remainder","length","blocks","h1","k1","c1","c2","j","i","x86Hash128","h2","h3","h4","k2","k3","k4","c3","c4","toString","slice","x64Hash128","h1Buff","Buffer","from","h1Reversed","reverse","h2Buff","h2Reversed","buff","buffer","allocUnsafe","version","x86","hash32","hash128","x64","inputValidation"],"sources":["/Users/ajitpawar/microsoft/bap/POCs/Scenario_checker_for_maker/mcs-scenario-checker-for-maker/node_modules/@azure/cosmos/src/utils/hashing/murmurHash.ts"],"sourcesContent":["// +----------------------------------------------------------------------+\n// | murmurHash3js.js v3.0.1 // https://github.com/pid/murmurHash3js\n// | A javascript implementation of MurmurHash3's x86 hashing algorithms. |\n// |----------------------------------------------------------------------|\n// | Copyright (c) 2012-2015 Karan Lyons                                       |\n// | https://github.com/karanlyons/murmurHash3.js/blob/c1778f75792abef7bdd74bc85d2d4e1a3d25cfe9/murmurHash3.js |\n// | Freely distributable under the MIT license.                          |\n// +----------------------------------------------------------------------+\n\n// PRIVATE FUNCTIONS\n// -----------------\n\nfunction _x86Multiply(m: number, n: number) {\n  //\n  // Given two 32bit ints, returns the two multiplied together as a\n  // 32bit int.\n  //\n\n  return (m & 0xffff) * n + ((((m >>> 16) * n) & 0xffff) << 16);\n}\n\nfunction _x86Rotl(m: number, n: number) {\n  //\n  // Given a 32bit int and an int representing a number of bit positions,\n  // returns the 32bit int rotated left by that number of positions.\n  //\n\n  return (m << n) | (m >>> (32 - n));\n}\n\nfunction _x86Fmix(h: number) {\n  //\n  // Given a block, returns murmurHash3's final x86 mix of that block.\n  //\n\n  h ^= h >>> 16;\n  h = _x86Multiply(h, 0x85ebca6b);\n  h ^= h >>> 13;\n  h = _x86Multiply(h, 0xc2b2ae35);\n  h ^= h >>> 16;\n\n  return h;\n}\n\nfunction _x64Add(m: number[], n: number[]) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // added together as a 64bit int (as an array of two 32bit ints).\n  //\n\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n\n  o[3] += m[3] + n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n\n  o[2] += m[2] + n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n\n  o[1] += m[1] + n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[0] += m[0] + n[0];\n  o[0] &= 0xffff;\n\n  return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]];\n}\n\nfunction _x64Multiply(m: number[], n: number[]) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // multiplied together as a 64bit int (as an array of two 32bit ints).\n  //\n\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n\n  o[3] += m[3] * n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n\n  o[2] += m[2] * n[3];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n\n  o[2] += m[3] * n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n\n  o[1] += m[1] * n[3];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[1] += m[2] * n[2];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[1] += m[3] * n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0];\n  o[0] &= 0xffff;\n\n  return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]];\n}\n\nfunction _x64Rotl(m: number[], n: number) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) rotated left by that number of positions.\n  //\n\n  n %= 64;\n\n  if (n === 32) {\n    return [m[1], m[0]];\n  } else if (n < 32) {\n    return [(m[0] << n) | (m[1] >>> (32 - n)), (m[1] << n) | (m[0] >>> (32 - n))];\n  } else {\n    n -= 32;\n    return [(m[1] << n) | (m[0] >>> (32 - n)), (m[0] << n) | (m[1] >>> (32 - n))];\n  }\n}\n\nfunction _x64LeftShift(m: number[], n: number) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) shifted left by that number of positions.\n  //\n\n  n %= 64;\n\n  if (n === 0) {\n    return m;\n  } else if (n < 32) {\n    return [(m[0] << n) | (m[1] >>> (32 - n)), m[1] << n];\n  } else {\n    return [m[1] << (n - 32), 0];\n  }\n}\n\nfunction _x64Xor(m: number[], n: number[]) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // xored together as a 64bit int (as an array of two 32bit ints).\n  //\n\n  return [m[0] ^ n[0], m[1] ^ n[1]];\n}\n\nfunction _x64Fmix(h: number[]) {\n  //\n  // Given a block, returns murmurHash3's final x64 mix of that block.\n  // (`[0, h[0] >>> 1]` is a 33 bit unsigned right shift. This is the\n  // only place where we need to right shift 64bit ints.)\n  //\n\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xff51afd7, 0xed558ccd]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xc4ceb9fe, 0x1a85ec53]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n\n  return h;\n}\n\n// PUBLIC FUNCTIONS\n// ----------------\n\nfunction x86Hash32(bytes: Buffer, seed?: number) {\n  //\n  // Given a string and an optional seed as an int, returns a 32 bit hash\n  // using the x86 flavor of MurmurHash3, as an unsigned int.\n  //\n  seed = seed || 0;\n\n  const remainder = bytes.length % 4;\n  const blocks = bytes.length - remainder;\n\n  let h1 = seed;\n\n  let k1 = 0;\n\n  const c1 = 0xcc9e2d51;\n  const c2 = 0x1b873593;\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 4) {\n    k1 = bytes[i] | (bytes[i + 1] << 8) | (bytes[i + 2] << 16) | (bytes[i + 3] << 24);\n\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n\n    h1 ^= k1;\n    h1 = _x86Rotl(h1, 13);\n    h1 = _x86Multiply(h1, 5) + 0xe6546b64;\n    j = i + 4;\n  }\n\n  k1 = 0;\n\n  switch (remainder) {\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n\n  h1 ^= bytes.length;\n  h1 = _x86Fmix(h1);\n\n  return h1 >>> 0;\n}\n\nfunction x86Hash128(bytes: Buffer, seed?: number) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x86 flavor of MurmurHash3, as an unsigned hex.\n  //\n\n  seed = seed || 0;\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n\n  let h1 = seed;\n  let h2 = seed;\n  let h3 = seed;\n  let h4 = seed;\n\n  let k1 = 0;\n  let k2 = 0;\n  let k3 = 0;\n  let k4 = 0;\n\n  const c1 = 0x239b961b;\n  const c2 = 0xab0e9789;\n  const c3 = 0x38b34ae5;\n  const c4 = 0xa1e38b93;\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = bytes[i] | (bytes[i + 1] << 8) | (bytes[i + 2] << 16) | (bytes[i + 3] << 24);\n    k2 = bytes[i + 4] | (bytes[i + 5] << 8) | (bytes[i + 6] << 16) | (bytes[i + 7] << 24);\n    k3 = bytes[i + 8] | (bytes[i + 9] << 8) | (bytes[i + 10] << 16) | (bytes[i + 11] << 24);\n    k4 = bytes[i + 12] | (bytes[i + 13] << 8) | (bytes[i + 14] << 16) | (bytes[i + 15] << 24);\n\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n    h1 ^= k1;\n\n    h1 = _x86Rotl(h1, 19);\n    h1 += h2;\n    h1 = _x86Multiply(h1, 5) + 0x561ccd1b;\n\n    k2 = _x86Multiply(k2, c2);\n    k2 = _x86Rotl(k2, 16);\n    k2 = _x86Multiply(k2, c3);\n    h2 ^= k2;\n\n    h2 = _x86Rotl(h2, 17);\n    h2 += h3;\n    h2 = _x86Multiply(h2, 5) + 0x0bcaa747;\n\n    k3 = _x86Multiply(k3, c3);\n    k3 = _x86Rotl(k3, 17);\n    k3 = _x86Multiply(k3, c4);\n    h3 ^= k3;\n\n    h3 = _x86Rotl(h3, 15);\n    h3 += h4;\n    h3 = _x86Multiply(h3, 5) + 0x96cd1c35;\n\n    k4 = _x86Multiply(k4, c4);\n    k4 = _x86Rotl(k4, 18);\n    k4 = _x86Multiply(k4, c1);\n    h4 ^= k4;\n\n    h4 = _x86Rotl(h4, 13);\n    h4 += h1;\n    h4 = _x86Multiply(h4, 5) + 0x32ac3b17;\n    j = i + 16;\n  }\n\n  k1 = 0;\n  k2 = 0;\n  k3 = 0;\n  k4 = 0;\n\n  switch (remainder) {\n    case 15:\n      k4 ^= bytes[j + 14] << 16;\n\n    case 14:\n      k4 ^= bytes[j + 13] << 8;\n\n    case 13:\n      k4 ^= bytes[j + 12];\n      k4 = _x86Multiply(k4, c4);\n      k4 = _x86Rotl(k4, 18);\n      k4 = _x86Multiply(k4, c1);\n      h4 ^= k4;\n\n    case 12:\n      k3 ^= bytes[j + 11] << 24;\n\n    case 11:\n      k3 ^= bytes[j + 10] << 16;\n\n    case 10:\n      k3 ^= bytes[j + 9] << 8;\n\n    case 9:\n      k3 ^= bytes[j + 8];\n      k3 = _x86Multiply(k3, c3);\n      k3 = _x86Rotl(k3, 17);\n      k3 = _x86Multiply(k3, c4);\n      h3 ^= k3;\n\n    case 8:\n      k2 ^= bytes[j + 7] << 24;\n\n    case 7:\n      k2 ^= bytes[j + 6] << 16;\n\n    case 6:\n      k2 ^= bytes[j + 5] << 8;\n\n    case 5:\n      k2 ^= bytes[j + 4];\n      k2 = _x86Multiply(k2, c2);\n      k2 = _x86Rotl(k2, 16);\n      k2 = _x86Multiply(k2, c3);\n      h2 ^= k2;\n\n    case 4:\n      k1 ^= bytes[j + 3] << 24;\n\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n\n  h1 ^= bytes.length;\n  h2 ^= bytes.length;\n  h3 ^= bytes.length;\n  h4 ^= bytes.length;\n\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n\n  h1 = _x86Fmix(h1);\n  h2 = _x86Fmix(h2);\n  h3 = _x86Fmix(h3);\n  h4 = _x86Fmix(h4);\n\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n\n  return (\n    (\"00000000\" + (h1 >>> 0).toString(16)).slice(-8) +\n    (\"00000000\" + (h2 >>> 0).toString(16)).slice(-8) +\n    (\"00000000\" + (h3 >>> 0).toString(16)).slice(-8) +\n    (\"00000000\" + (h4 >>> 0).toString(16)).slice(-8)\n  );\n}\n\nfunction x64Hash128(bytes: Buffer, seed?: number) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x64 flavor of MurmurHash3, as an unsigned hex.\n  //\n  seed = seed || 0;\n\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n\n  let h1 = [0, seed];\n  let h2 = [0, seed];\n\n  let k1 = [0, 0];\n  let k2 = [0, 0];\n\n  const c1 = [0x87c37b91, 0x114253d5];\n  const c2 = [0x4cf5ad43, 0x2745937f];\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = [\n      bytes[i + 4] | (bytes[i + 5] << 8) | (bytes[i + 6] << 16) | (bytes[i + 7] << 24),\n      bytes[i] | (bytes[i + 1] << 8) | (bytes[i + 2] << 16) | (bytes[i + 3] << 24),\n    ];\n    k2 = [\n      bytes[i + 12] | (bytes[i + 13] << 8) | (bytes[i + 14] << 16) | (bytes[i + 15] << 24),\n      bytes[i + 8] | (bytes[i + 9] << 8) | (bytes[i + 10] << 16) | (bytes[i + 11] << 24),\n    ];\n\n    k1 = _x64Multiply(k1, c1);\n    k1 = _x64Rotl(k1, 31);\n    k1 = _x64Multiply(k1, c2);\n    h1 = _x64Xor(h1, k1);\n\n    h1 = _x64Rotl(h1, 27);\n    h1 = _x64Add(h1, h2);\n    h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 0x52dce729]);\n\n    k2 = _x64Multiply(k2, c2);\n    k2 = _x64Rotl(k2, 33);\n    k2 = _x64Multiply(k2, c1);\n    h2 = _x64Xor(h2, k2);\n\n    h2 = _x64Rotl(h2, 31);\n    h2 = _x64Add(h2, h1);\n    h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 0x38495ab5]);\n    j = i + 16;\n  }\n\n  k1 = [0, 0];\n  k2 = [0, 0];\n\n  switch (remainder) {\n    case 15:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 14]], 48));\n\n    case 14:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 13]], 40));\n\n    case 13:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 12]], 32));\n\n    case 12:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 11]], 24));\n\n    case 11:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 10]], 16));\n\n    case 10:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 9]], 8));\n\n    case 9:\n      k2 = _x64Xor(k2, [0, bytes[j + 8]]);\n      k2 = _x64Multiply(k2, c2);\n      k2 = _x64Rotl(k2, 33);\n      k2 = _x64Multiply(k2, c1);\n      h2 = _x64Xor(h2, k2);\n\n    case 8:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 7]], 56));\n\n    case 7:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 6]], 48));\n\n    case 6:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 5]], 40));\n\n    case 5:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 4]], 32));\n\n    case 4:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 3]], 24));\n\n    case 3:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 2]], 16));\n\n    case 2:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 1]], 8));\n\n    case 1:\n      k1 = _x64Xor(k1, [0, bytes[j]]);\n      k1 = _x64Multiply(k1, c1);\n      k1 = _x64Rotl(k1, 31);\n      k1 = _x64Multiply(k1, c2);\n      h1 = _x64Xor(h1, k1);\n  }\n\n  h1 = _x64Xor(h1, [0, bytes.length]);\n  h2 = _x64Xor(h2, [0, bytes.length]);\n\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n\n  h1 = _x64Fmix(h1);\n  h2 = _x64Fmix(h2);\n\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n\n  // Here we reverse h1 and h2 in Cosmos\n  // This is an implementation detail and not part of the public spec\n  const h1Buff = Buffer.from(\n    (\"00000000\" + (h1[0] >>> 0).toString(16)).slice(-8) +\n      (\"00000000\" + (h1[1] >>> 0).toString(16)).slice(-8),\n    \"hex\",\n  );\n  const h1Reversed = reverse(h1Buff).toString(\"hex\");\n  const h2Buff = Buffer.from(\n    (\"00000000\" + (h2[0] >>> 0).toString(16)).slice(-8) +\n      (\"00000000\" + (h2[1] >>> 0).toString(16)).slice(-8),\n    \"hex\",\n  );\n  const h2Reversed = reverse(h2Buff).toString(\"hex\");\n  return h1Reversed + h2Reversed;\n}\n\nexport function reverse(buff: Buffer) {\n  const buffer = Buffer.allocUnsafe(buff.length);\n\n  for (let i = 0, j = buff.length - 1; i <= j; ++i, --j) {\n    buffer[i] = buff[j];\n    buffer[j] = buff[i];\n  }\n  return buffer;\n}\n\nexport default {\n  version: \"3.0.0\",\n  x86: {\n    hash32: x86Hash32,\n    hash128: x86Hash128,\n  },\n  x64: {\n    hash128: x64Hash128,\n  },\n  inputValidation: true,\n};\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA,SAASA,YAAYA,CAACC,CAAS,EAAEC,CAAS;EACxC;EACA;EACA;EACA;EAEA,OAAO,CAACD,CAAC,GAAG,MAAM,IAAIC,CAAC,IAAI,CAAE,CAACD,CAAC,KAAK,EAAE,IAAIC,CAAC,GAAI,MAAM,KAAK,EAAE,CAAC;AAC/D;AAEA,SAASC,QAAQA,CAACF,CAAS,EAAEC,CAAS;EACpC;EACA;EACA;EACA;EAEA,OAAQD,CAAC,IAAIC,CAAC,GAAKD,CAAC,KAAM,EAAE,GAAGC,CAAG;AACpC;AAEA,SAASE,QAAQA,CAACC,CAAS;EACzB;EACA;EACA;EAEAA,CAAC,IAAIA,CAAC,KAAK,EAAE;EACbA,CAAC,GAAGL,YAAY,CAACK,CAAC,EAAE,UAAU,CAAC;EAC/BA,CAAC,IAAIA,CAAC,KAAK,EAAE;EACbA,CAAC,GAAGL,YAAY,CAACK,CAAC,EAAE,UAAU,CAAC;EAC/BA,CAAC,IAAIA,CAAC,KAAK,EAAE;EAEb,OAAOA,CAAC;AACV;AAEA,SAASC,OAAOA,CAACL,CAAW,EAAEC,CAAW;EACvC;EACA;EACA;EACA;EAEAD,CAAC,GAAG,CAACA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC;EAC5DC,CAAC,GAAG,CAACA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC;EAC5D,MAAMK,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EAEtBA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEd,OAAO,CAAEA,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,GAAIA,CAAC,CAAC,CAAC,CAAC,EAAGA,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,GAAIA,CAAC,CAAC,CAAC,CAAC,CAAC;AACnD;AAEA,SAASC,YAAYA,CAACP,CAAW,EAAEC,CAAW;EAC5C;EACA;EACA;EACA;EAEAD,CAAC,GAAG,CAACA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC;EAC5DC,CAAC,GAAG,CAACA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,EAAEA,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC;EAC5D,MAAMK,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EAEtBA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EACnBK,CAAC,CAAC,CAAC,CAAC,IAAIA,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE;EACnBA,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEdA,CAAC,CAAC,CAAC,CAAC,IAAIN,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC,GAAGD,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC,GAAGD,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC,GAAGD,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC;EAC7DK,CAAC,CAAC,CAAC,CAAC,IAAI,MAAM;EAEd,OAAO,CAAEA,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,GAAIA,CAAC,CAAC,CAAC,CAAC,EAAGA,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,GAAIA,CAAC,CAAC,CAAC,CAAC,CAAC;AACnD;AAEA,SAASE,QAAQA,CAACR,CAAW,EAAEC,CAAS;EACtC;EACA;EACA;EACA;EACA;EAEAA,CAAC,IAAI,EAAE;EAEP,IAAIA,CAAC,KAAK,EAAE,EAAE;IACZ,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,EAAEA,CAAC,CAAC,CAAC,CAAC,CAAC;EACrB,CAAC,MAAM,IAAIC,CAAC,GAAG,EAAE,EAAE;IACjB,OAAO,CAAED,CAAC,CAAC,CAAC,CAAC,IAAIC,CAAC,GAAKD,CAAC,CAAC,CAAC,CAAC,KAAM,EAAE,GAAGC,CAAG,EAAGD,CAAC,CAAC,CAAC,CAAC,IAAIC,CAAC,GAAKD,CAAC,CAAC,CAAC,CAAC,KAAM,EAAE,GAAGC,CAAG,CAAC;EAC/E,CAAC,MAAM;IACLA,CAAC,IAAI,EAAE;IACP,OAAO,CAAED,CAAC,CAAC,CAAC,CAAC,IAAIC,CAAC,GAAKD,CAAC,CAAC,CAAC,CAAC,KAAM,EAAE,GAAGC,CAAG,EAAGD,CAAC,CAAC,CAAC,CAAC,IAAIC,CAAC,GAAKD,CAAC,CAAC,CAAC,CAAC,KAAM,EAAE,GAAGC,CAAG,CAAC;EAC/E;AACF;AAEA,SAASQ,aAAaA,CAACT,CAAW,EAAEC,CAAS;EAC3C;EACA;EACA;EACA;EACA;EAEAA,CAAC,IAAI,EAAE;EAEP,IAAIA,CAAC,KAAK,CAAC,EAAE;IACX,OAAOD,CAAC;EACV,CAAC,MAAM,IAAIC,CAAC,GAAG,EAAE,EAAE;IACjB,OAAO,CAAED,CAAC,CAAC,CAAC,CAAC,IAAIC,CAAC,GAAKD,CAAC,CAAC,CAAC,CAAC,KAAM,EAAE,GAAGC,CAAG,EAAED,CAAC,CAAC,CAAC,CAAC,IAAIC,CAAC,CAAC;EACvD,CAAC,MAAM;IACL,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,IAAKC,CAAC,GAAG,EAAG,EAAE,CAAC,CAAC;EAC9B;AACF;AAEA,SAASS,OAAOA,CAACV,CAAW,EAAEC,CAAW;EACvC;EACA;EACA;EACA;EAEA,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC,EAAED,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC,CAAC;AACnC;AAEA,SAASU,QAAQA,CAACP,CAAW;EAC3B;EACA;EACA;EACA;EACA;EAEAA,CAAC,GAAGM,OAAO,CAACN,CAAC,EAAE,CAAC,CAAC,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;EAC/BA,CAAC,GAAGG,YAAY,CAACH,CAAC,EAAE,CAAC,UAAU,EAAE,UAAU,CAAC,CAAC;EAC7CA,CAAC,GAAGM,OAAO,CAACN,CAAC,EAAE,CAAC,CAAC,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;EAC/BA,CAAC,GAAGG,YAAY,CAACH,CAAC,EAAE,CAAC,UAAU,EAAE,UAAU,CAAC,CAAC;EAC7CA,CAAC,GAAGM,OAAO,CAACN,CAAC,EAAE,CAAC,CAAC,EAAEA,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;EAE/B,OAAOA,CAAC;AACV;AAEA;AACA;AAEA,SAASQ,SAASA,CAACC,KAAa,EAAEC,IAAa;EAC7C;EACA;EACA;EACA;EACAA,IAAI,GAAGA,IAAI,IAAI,CAAC;EAEhB,MAAMC,SAAS,GAAGF,KAAK,CAACG,MAAM,GAAG,CAAC;EAClC,MAAMC,MAAM,GAAGJ,KAAK,CAACG,MAAM,GAAGD,SAAS;EAEvC,IAAIG,EAAE,GAAGJ,IAAI;EAEb,IAAIK,EAAE,GAAG,CAAC;EAEV,MAAMC,EAAE,GAAG,UAAU;EACrB,MAAMC,EAAE,GAAG,UAAU;EACrB,IAAIC,CAAC,GAAG,CAAC;EAET,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,MAAM,EAAEM,CAAC,GAAGA,CAAC,GAAG,CAAC,EAAE;IACrCJ,EAAE,GAAGN,KAAK,CAACU,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG;IAEjFJ,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEC,EAAE,CAAC;IACzBD,EAAE,GAAGjB,QAAQ,CAACiB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEE,EAAE,CAAC;IAEzBH,EAAE,IAAIC,EAAE;IACRD,EAAE,GAAGhB,QAAQ,CAACgB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGnB,YAAY,CAACmB,EAAE,EAAE,CAAC,CAAC,GAAG,UAAU;IACrCI,CAAC,GAAGC,CAAC,GAAG,CAAC;EACX;EAEAJ,EAAE,GAAG,CAAC;EAEN,QAAQJ,SAAS;IACf,KAAK,CAAC;MACJI,EAAE,IAAIN,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE;IAE1B,KAAK,CAAC;MACJH,EAAE,IAAIN,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC;IAEzB,KAAK,CAAC;MACJH,EAAE,IAAIN,KAAK,CAACS,CAAC,CAAC;MACdH,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEC,EAAE,CAAC;MACzBD,EAAE,GAAGjB,QAAQ,CAACiB,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEE,EAAE,CAAC;MACzBH,EAAE,IAAIC,EAAE;EACZ;EAEAD,EAAE,IAAIL,KAAK,CAACG,MAAM;EAClBE,EAAE,GAAGf,QAAQ,CAACe,EAAE,CAAC;EAEjB,OAAOA,EAAE,KAAK,CAAC;AACjB;AAEA,SAASM,UAAUA,CAACX,KAAa,EAAEC,IAAa;EAC9C;EACA;EACA;EACA;EAEAA,IAAI,GAAGA,IAAI,IAAI,CAAC;EAChB,MAAMC,SAAS,GAAGF,KAAK,CAACG,MAAM,GAAG,EAAE;EACnC,MAAMC,MAAM,GAAGJ,KAAK,CAACG,MAAM,GAAGD,SAAS;EAEvC,IAAIG,EAAE,GAAGJ,IAAI;EACb,IAAIW,EAAE,GAAGX,IAAI;EACb,IAAIY,EAAE,GAAGZ,IAAI;EACb,IAAIa,EAAE,GAAGb,IAAI;EAEb,IAAIK,EAAE,GAAG,CAAC;EACV,IAAIS,EAAE,GAAG,CAAC;EACV,IAAIC,EAAE,GAAG,CAAC;EACV,IAAIC,EAAE,GAAG,CAAC;EAEV,MAAMV,EAAE,GAAG,UAAU;EACrB,MAAMC,EAAE,GAAG,UAAU;EACrB,MAAMU,EAAE,GAAG,UAAU;EACrB,MAAMC,EAAE,GAAG,UAAU;EACrB,IAAIV,CAAC,GAAG,CAAC;EAET,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,MAAM,EAAEM,CAAC,GAAGA,CAAC,GAAG,EAAE,EAAE;IACtCJ,EAAE,GAAGN,KAAK,CAACU,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG;IACjFK,EAAE,GAAGf,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG;IACrFM,EAAE,GAAGhB,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG;IACvFO,EAAE,GAAGjB,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG;IAEzFJ,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEC,EAAE,CAAC;IACzBD,EAAE,GAAGjB,QAAQ,CAACiB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEE,EAAE,CAAC;IACzBH,EAAE,IAAIC,EAAE;IAERD,EAAE,GAAGhB,QAAQ,CAACgB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,IAAIO,EAAE;IACRP,EAAE,GAAGnB,YAAY,CAACmB,EAAE,EAAE,CAAC,CAAC,GAAG,UAAU;IAErCU,EAAE,GAAG7B,YAAY,CAAC6B,EAAE,EAAEP,EAAE,CAAC;IACzBO,EAAE,GAAG1B,QAAQ,CAAC0B,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAG7B,YAAY,CAAC6B,EAAE,EAAEG,EAAE,CAAC;IACzBN,EAAE,IAAIG,EAAE;IAERH,EAAE,GAAGvB,QAAQ,CAACuB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,IAAIC,EAAE;IACRD,EAAE,GAAG1B,YAAY,CAAC0B,EAAE,EAAE,CAAC,CAAC,GAAG,UAAU;IAErCI,EAAE,GAAG9B,YAAY,CAAC8B,EAAE,EAAEE,EAAE,CAAC;IACzBF,EAAE,GAAG3B,QAAQ,CAAC2B,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAG9B,YAAY,CAAC8B,EAAE,EAAEG,EAAE,CAAC;IACzBN,EAAE,IAAIG,EAAE;IAERH,EAAE,GAAGxB,QAAQ,CAACwB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,IAAIC,EAAE;IACRD,EAAE,GAAG3B,YAAY,CAAC2B,EAAE,EAAE,CAAC,CAAC,GAAG,UAAU;IAErCI,EAAE,GAAG/B,YAAY,CAAC+B,EAAE,EAAEE,EAAE,CAAC;IACzBF,EAAE,GAAG5B,QAAQ,CAAC4B,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAG/B,YAAY,CAAC+B,EAAE,EAAEV,EAAE,CAAC;IACzBO,EAAE,IAAIG,EAAE;IAERH,EAAE,GAAGzB,QAAQ,CAACyB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,IAAIT,EAAE;IACRS,EAAE,GAAG5B,YAAY,CAAC4B,EAAE,EAAE,CAAC,CAAC,GAAG,UAAU;IACrCL,CAAC,GAAGC,CAAC,GAAG,EAAE;EACZ;EAEAJ,EAAE,GAAG,CAAC;EACNS,EAAE,GAAG,CAAC;EACNC,EAAE,GAAG,CAAC;EACNC,EAAE,GAAG,CAAC;EAEN,QAAQf,SAAS;IACf,KAAK,EAAE;MACLe,EAAE,IAAIjB,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,IAAI,EAAE;IAE3B,KAAK,EAAE;MACLQ,EAAE,IAAIjB,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,IAAI,CAAC;IAE1B,KAAK,EAAE;MACLQ,EAAE,IAAIjB,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC;MACnBQ,EAAE,GAAG/B,YAAY,CAAC+B,EAAE,EAAEE,EAAE,CAAC;MACzBF,EAAE,GAAG5B,QAAQ,CAAC4B,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAG/B,YAAY,CAAC+B,EAAE,EAAEV,EAAE,CAAC;MACzBO,EAAE,IAAIG,EAAE;IAEV,KAAK,EAAE;MACLD,EAAE,IAAIhB,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,IAAI,EAAE;IAE3B,KAAK,EAAE;MACLO,EAAE,IAAIhB,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,IAAI,EAAE;IAE3B,KAAK,EAAE;MACLO,EAAE,IAAIhB,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC;IAEzB,KAAK,CAAC;MACJO,EAAE,IAAIhB,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC;MAClBO,EAAE,GAAG9B,YAAY,CAAC8B,EAAE,EAAEE,EAAE,CAAC;MACzBF,EAAE,GAAG3B,QAAQ,CAAC2B,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAG9B,YAAY,CAAC8B,EAAE,EAAEG,EAAE,CAAC;MACzBN,EAAE,IAAIG,EAAE;IAEV,KAAK,CAAC;MACJD,EAAE,IAAIf,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE;IAE1B,KAAK,CAAC;MACJM,EAAE,IAAIf,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE;IAE1B,KAAK,CAAC;MACJM,EAAE,IAAIf,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC;IAEzB,KAAK,CAAC;MACJM,EAAE,IAAIf,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC;MAClBM,EAAE,GAAG7B,YAAY,CAAC6B,EAAE,EAAEP,EAAE,CAAC;MACzBO,EAAE,GAAG1B,QAAQ,CAAC0B,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAG7B,YAAY,CAAC6B,EAAE,EAAEG,EAAE,CAAC;MACzBN,EAAE,IAAIG,EAAE;IAEV,KAAK,CAAC;MACJT,EAAE,IAAIN,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE;IAE1B,KAAK,CAAC;MACJH,EAAE,IAAIN,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE;IAE1B,KAAK,CAAC;MACJH,EAAE,IAAIN,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC;IAEzB,KAAK,CAAC;MACJH,EAAE,IAAIN,KAAK,CAACS,CAAC,CAAC;MACdH,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEC,EAAE,CAAC;MACzBD,EAAE,GAAGjB,QAAQ,CAACiB,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAGpB,YAAY,CAACoB,EAAE,EAAEE,EAAE,CAAC;MACzBH,EAAE,IAAIC,EAAE;EACZ;EAEAD,EAAE,IAAIL,KAAK,CAACG,MAAM;EAClBS,EAAE,IAAIZ,KAAK,CAACG,MAAM;EAClBU,EAAE,IAAIb,KAAK,CAACG,MAAM;EAClBW,EAAE,IAAId,KAAK,CAACG,MAAM;EAElBE,EAAE,IAAIO,EAAE;EACRP,EAAE,IAAIQ,EAAE;EACRR,EAAE,IAAIS,EAAE;EACRF,EAAE,IAAIP,EAAE;EACRQ,EAAE,IAAIR,EAAE;EACRS,EAAE,IAAIT,EAAE;EAERA,EAAE,GAAGf,QAAQ,CAACe,EAAE,CAAC;EACjBO,EAAE,GAAGtB,QAAQ,CAACsB,EAAE,CAAC;EACjBC,EAAE,GAAGvB,QAAQ,CAACuB,EAAE,CAAC;EACjBC,EAAE,GAAGxB,QAAQ,CAACwB,EAAE,CAAC;EAEjBT,EAAE,IAAIO,EAAE;EACRP,EAAE,IAAIQ,EAAE;EACRR,EAAE,IAAIS,EAAE;EACRF,EAAE,IAAIP,EAAE;EACRQ,EAAE,IAAIR,EAAE;EACRS,EAAE,IAAIT,EAAE;EAER,OACE,CAAC,UAAU,GAAG,CAACA,EAAE,KAAK,CAAC,EAAEe,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,GAChD,CAAC,UAAU,GAAG,CAACT,EAAE,KAAK,CAAC,EAAEQ,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,GAChD,CAAC,UAAU,GAAG,CAACR,EAAE,KAAK,CAAC,EAAEO,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,GAChD,CAAC,UAAU,GAAG,CAACP,EAAE,KAAK,CAAC,EAAEM,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC;AAEpD;AAEA,SAASC,UAAUA,CAACtB,KAAa,EAAEC,IAAa;EAC9C;EACA;EACA;EACA;EACAA,IAAI,GAAGA,IAAI,IAAI,CAAC;EAEhB,MAAMC,SAAS,GAAGF,KAAK,CAACG,MAAM,GAAG,EAAE;EACnC,MAAMC,MAAM,GAAGJ,KAAK,CAACG,MAAM,GAAGD,SAAS;EAEvC,IAAIG,EAAE,GAAG,CAAC,CAAC,EAAEJ,IAAI,CAAC;EAClB,IAAIW,EAAE,GAAG,CAAC,CAAC,EAAEX,IAAI,CAAC;EAElB,IAAIK,EAAE,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;EACf,IAAIS,EAAE,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;EAEf,MAAMR,EAAE,GAAG,CAAC,UAAU,EAAE,UAAU,CAAC;EACnC,MAAMC,EAAE,GAAG,CAAC,UAAU,EAAE,UAAU,CAAC;EACnC,IAAIC,CAAC,GAAG,CAAC;EAET,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,MAAM,EAAEM,CAAC,GAAGA,CAAC,GAAG,EAAE,EAAE;IACtCJ,EAAE,GAAG,CACHN,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,EAChFV,KAAK,CAACU,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,EAAG,CAC7E;IACDK,EAAE,GAAG,CACHf,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG,EACpFV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,GAAIV,KAAK,CAACU,CAAC,GAAG,CAAC,CAAC,IAAI,CAAE,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG,GAAIV,KAAK,CAACU,CAAC,GAAG,EAAE,CAAC,IAAI,EAAG,CACnF;IAEDJ,EAAE,GAAGZ,YAAY,CAACY,EAAE,EAAEC,EAAE,CAAC;IACzBD,EAAE,GAAGX,QAAQ,CAACW,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGZ,YAAY,CAACY,EAAE,EAAEE,EAAE,CAAC;IACzBH,EAAE,GAAGR,OAAO,CAACQ,EAAE,EAAEC,EAAE,CAAC;IAEpBD,EAAE,GAAGV,QAAQ,CAACU,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGb,OAAO,CAACa,EAAE,EAAEO,EAAE,CAAC;IACpBP,EAAE,GAAGb,OAAO,CAACE,YAAY,CAACW,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,UAAU,CAAC,CAAC;IAEvDU,EAAE,GAAGrB,YAAY,CAACqB,EAAE,EAAEP,EAAE,CAAC;IACzBO,EAAE,GAAGpB,QAAQ,CAACoB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGrB,YAAY,CAACqB,EAAE,EAAER,EAAE,CAAC;IACzBK,EAAE,GAAGf,OAAO,CAACe,EAAE,EAAEG,EAAE,CAAC;IAEpBH,EAAE,GAAGjB,QAAQ,CAACiB,EAAE,EAAE,EAAE,CAAC;IACrBA,EAAE,GAAGpB,OAAO,CAACoB,EAAE,EAAEP,EAAE,CAAC;IACpBO,EAAE,GAAGpB,OAAO,CAACE,YAAY,CAACkB,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,UAAU,CAAC,CAAC;IACvDH,CAAC,GAAGC,CAAC,GAAG,EAAE;EACZ;EAEAJ,EAAE,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;EACXS,EAAE,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;EAEX,QAAQb,SAAS;IACf,KAAK,EAAE;MACLa,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAEnB,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAEzD,KAAK,EAAE;MACLM,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAEnB,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAEzD,KAAK,EAAE;MACLM,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAEnB,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAEzD,KAAK,EAAE;MACLM,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAEnB,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAEzD,KAAK,EAAE;MACLM,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAEnB,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAEzD,KAAK,EAAE;MACLM,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAEnB,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAEvD,KAAK,CAAC;MACJM,EAAE,GAAGlB,OAAO,CAACkB,EAAE,EAAE,CAAC,CAAC,EAAEf,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;MACnCM,EAAE,GAAGrB,YAAY,CAACqB,EAAE,EAAEP,EAAE,CAAC;MACzBO,EAAE,GAAGpB,QAAQ,CAACoB,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAGrB,YAAY,CAACqB,EAAE,EAAER,EAAE,CAAC;MACzBK,EAAE,GAAGf,OAAO,CAACe,EAAE,EAAEG,EAAE,CAAC;IAEtB,KAAK,CAAC;MACJT,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAExD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAExD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAExD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAExD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAExD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IAExD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAEV,aAAa,CAAC,CAAC,CAAC,EAAEI,KAAK,CAACS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAEvD,KAAK,CAAC;MACJH,EAAE,GAAGT,OAAO,CAACS,EAAE,EAAE,CAAC,CAAC,EAAEN,KAAK,CAACS,CAAC,CAAC,CAAC,CAAC;MAC/BH,EAAE,GAAGZ,YAAY,CAACY,EAAE,EAAEC,EAAE,CAAC;MACzBD,EAAE,GAAGX,QAAQ,CAACW,EAAE,EAAE,EAAE,CAAC;MACrBA,EAAE,GAAGZ,YAAY,CAACY,EAAE,EAAEE,EAAE,CAAC;MACzBH,EAAE,GAAGR,OAAO,CAACQ,EAAE,EAAEC,EAAE,CAAC;EACxB;EAEAD,EAAE,GAAGR,OAAO,CAACQ,EAAE,EAAE,CAAC,CAAC,EAAEL,KAAK,CAACG,MAAM,CAAC,CAAC;EACnCS,EAAE,GAAGf,OAAO,CAACe,EAAE,EAAE,CAAC,CAAC,EAAEZ,KAAK,CAACG,MAAM,CAAC,CAAC;EAEnCE,EAAE,GAAGb,OAAO,CAACa,EAAE,EAAEO,EAAE,CAAC;EACpBA,EAAE,GAAGpB,OAAO,CAACoB,EAAE,EAAEP,EAAE,CAAC;EAEpBA,EAAE,GAAGP,QAAQ,CAACO,EAAE,CAAC;EACjBO,EAAE,GAAGd,QAAQ,CAACc,EAAE,CAAC;EAEjBP,EAAE,GAAGb,OAAO,CAACa,EAAE,EAAEO,EAAE,CAAC;EACpBA,EAAE,GAAGpB,OAAO,CAACoB,EAAE,EAAEP,EAAE,CAAC;EAEpB;EACA;EACA,MAAMkB,MAAM,GAAGC,MAAM,CAACC,IAAI,CACxB,CAAC,UAAU,GAAG,CAACpB,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,EAAEe,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,GACjD,CAAC,UAAU,GAAG,CAAChB,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,EAAEe,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,EACrD,KAAK,CACN;EACD,MAAMK,UAAU,GAAGC,OAAO,CAACJ,MAAM,CAAC,CAACH,QAAQ,CAAC,KAAK,CAAC;EAClD,MAAMQ,MAAM,GAAGJ,MAAM,CAACC,IAAI,CACxB,CAAC,UAAU,GAAG,CAACb,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,EAAEQ,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,GACjD,CAAC,UAAU,GAAG,CAACT,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,EAAEQ,QAAQ,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,CAAC,CAAC,CAAC,EACrD,KAAK,CACN;EACD,MAAMQ,UAAU,GAAGF,OAAO,CAACC,MAAM,CAAC,CAACR,QAAQ,CAAC,KAAK,CAAC;EAClD,OAAOM,UAAU,GAAGG,UAAU;AAChC;AAEA,OAAM,SAAUF,OAAOA,CAACG,IAAY;EAClC,MAAMC,MAAM,GAAGP,MAAM,CAACQ,WAAW,CAACF,IAAI,CAAC3B,MAAM,CAAC;EAE9C,KAAK,IAAIO,CAAC,GAAG,CAAC,EAAED,CAAC,GAAGqB,IAAI,CAAC3B,MAAM,GAAG,CAAC,EAAEO,CAAC,IAAID,CAAC,EAAE,EAAEC,CAAC,EAAE,EAAED,CAAC,EAAE;IACrDsB,MAAM,CAACrB,CAAC,CAAC,GAAGoB,IAAI,CAACrB,CAAC,CAAC;IACnBsB,MAAM,CAACtB,CAAC,CAAC,GAAGqB,IAAI,CAACpB,CAAC,CAAC;EACrB;EACA,OAAOqB,MAAM;AACf;AAEA,eAAe;EACbE,OAAO,EAAE,OAAO;EAChBC,GAAG,EAAE;IACHC,MAAM,EAAEpC,SAAS;IACjBqC,OAAO,EAAEzB;GACV;EACD0B,GAAG,EAAE;IACHD,OAAO,EAAEd;GACV;EACDgB,eAAe,EAAE;CAClB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}